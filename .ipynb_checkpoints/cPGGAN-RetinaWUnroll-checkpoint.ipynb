{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "#%matplotlib inline\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "import torch.nn.functional as F\n",
    "from skimage import io, transform\n",
    "\n",
    "import copy\n",
    "from torch.autograd import grad\n",
    "import torchvision\n",
    "import pandas as pd\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "writer = SummaryWriter('runs/cPGGANs')\n",
    "\n",
    "# import torchvision.transforms.functional as TF\n",
    "\n",
    "from my_utils import GeneratorC, DiscriminatorC, save_model, save_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loader(x: torch.Tensor, y: torch.Tensor, batch_size=5, numThreads = 1) -> torch.utils.data.DataLoader:\n",
    "    \"\"\"Fetches a DataLoader, which is built into PyTorch, and provides a\n",
    "    convenient (and efficient) method for sampling.\n",
    "\n",
    "    :param x: (torch.Tensor) inputs\n",
    "    :param y: (torch.Tensor) labels\n",
    "    :param batch_size: (int)\n",
    "    \"\"\"\n",
    "    dataset = torch.utils.data.TensorDataset(x, y)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset, num_workers=numThreads, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    return data_loader\n",
    "\n",
    "def unroll(data):\n",
    "    gp_lambda = 10\n",
    "    fake_label = 0\n",
    "        ############################\n",
    "    # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "    ###########################\n",
    "    ## Train with all-real batch\n",
    "    netDK.zero_grad()\n",
    "    # Format batch\n",
    "    real_cpu = data[0].to(device)\n",
    "    y_onehot = turn_label_to_one_hot(data[1].to(device))\n",
    "    b_size = real_cpu.size(0)\n",
    "    label = torch.full((b_size,), fake_label, device=device)\n",
    "    # Forward pass real batch through D\n",
    "    output = netDK(real_cpu, y_onehot, flag = flag, alpha=alpha).view(-1)\n",
    "    # Calculate loss on all-real batch\n",
    "#     errD_real = criterion(output, label)\n",
    "    errD_real = - output.mean()\n",
    "    # Calculate gradients for D in backward pass\n",
    "    errD_real.backward()\n",
    "    D_x = output.mean().item()\n",
    "\n",
    "\n",
    "    ## Train with all-fake batch\n",
    "    # Generate batch of latent vectors\n",
    "    z_fake, y_onehot_fake = gen_fake(b_size)\n",
    "#     noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "    # Generate fake image batch with G\n",
    "    fake = netG(z_fake, flag = flag, alpha=alpha)\n",
    "    label.fill_(fake_label)\n",
    "    # Classify all fake batch with D\n",
    "    output = netDK(fake.detach(), y_onehot_fake, flag = flag, alpha=alpha).view(-1)\n",
    "    # Calculate D's loss on the all-fake batch\n",
    "    errD_fake = output.mean() #+ 10.0 * get_gp(real_cpu, fake, netDK, y_onehot_fake, flag = flag, alpha=alpha )\n",
    "    # Calculate the gradients for this batch\n",
    "    errD_fake.backward()\n",
    "    D_G_z1 = output.mean().item()\n",
    "    # Add the gradients from the all-real and all-fake batches\n",
    "    errD = errD_real + errD_fake\n",
    "    # Update D\n",
    "    optimizerDK.step()\n",
    "\n",
    "def copy_unroll(data, K=1):\n",
    "    optimizerDK.load_state_dict(optimizerD.state_dict())\n",
    "#     optimizerDK = copy.deepcopy(optimizerD)\n",
    "    netDK.load_state_dict(netD.state_dict())\n",
    "    for i in range(K):\n",
    "        unroll(data)  \n",
    "        \n",
    "def get_gp(x, fake_x, nn, y_onehot_fake, flag , alpha):\n",
    "    batch = x.shape[0]\n",
    "    alpha = torch.rand(batch, 1, 1, 1).to(device)\n",
    "\n",
    "    x_hat = alpha * x.detach() + (1 - alpha) * fake_x.detach()\n",
    "    x_hat.requires_grad_(True)\n",
    "\n",
    "    pred_hat = nn(x_hat, y_onehot_fake, flag = flag, alpha=alpha)\n",
    "    gradients = grad(outputs=pred_hat, inputs=x_hat,\n",
    "                     grad_outputs=torch.ones(pred_hat.size()).to(device),\n",
    "                     create_graph=True, retain_graph=True, only_inputs=True)[0]\n",
    "\n",
    "    grad_norm = gradients.view(batch, -1).norm(2, dim=1)\n",
    "    return grad_norm.sub(750).pow(2).mean()/(750**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of workers for dataloader\n",
    "workers = 8\n",
    "\n",
    "# Batch size during training\n",
    "batch_size = 16\n",
    "\n",
    "# Spatial size of training images. All images will be resized to this\n",
    "#   size using a transformer.\n",
    "image_size = 64\n",
    "cropSec = 64\n",
    "\n",
    "# Number of channels in the training images. For color images this is 3\n",
    "nc = 3\n",
    "\n",
    "# Size of z latent vector (i.e. size of generator input)\n",
    "nz = 512\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.0001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.0\n",
    "\n",
    "# Number of GPUs available. Use 0 for CPU mode.\n",
    "ngpu = 1\n",
    "nb_digits = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LOAD DATA\n",
    "dataloader = get_data_loader(x_train, y_train, batch_size, numThreads=workers)\n",
    "\n",
    "# Decide which device we want to run on\n",
    "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")\n",
    "\n",
    "# Plot some training images\n",
    "real_batch = next(iter(dataloader))\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Training Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=2, normalize=True).cpu(),(1,2,0)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "a_step = dataloader.dataset.tensors[0].shape[0] // batch_size\n",
    "num_epochs = 800000//dataloader.dataset.tensors[0].shape[0]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def turn_label_to_one_hot(y):\n",
    "    y_onehot = torch.FloatTensor(y.size()[0], nb_digits).to(device)\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, y.view(-1,1),1)\n",
    "    return y_onehot\n",
    "\n",
    "def gen_fake(b_size):\n",
    "    z = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "    y = torch.LongTensor(b_size,1).random_().to(device) % nb_digits\n",
    "    y_onehot = torch.FloatTensor(b_size, nb_digits).to(device)\n",
    "    y_onehot.zero_()\n",
    "    y_onehot.scatter_(1, y, 1)\n",
    "    return torch.cat((z,y_onehot.view(-1,nb_digits,1,1)),dim=1), y_onehot.scatter_(1, y, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the generator\n",
    "netG = GeneratorC(ngpu, nb_digits).to(device)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netG = nn.DataParallel(netG, list(range(ngpu)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Discriminator\n",
    "netD = DiscriminatorC(ngpu, nb_digits).to(device)\n",
    "netDK = copy.deepcopy(netD)\n",
    "\n",
    "# Handle multi-gpu if desired\n",
    "if (device.type == 'cuda') and (ngpu > 1):\n",
    "    netD = nn.DataParallel(netD, list(range(ngpu)))\n",
    "    netDK = copy.deepcopy(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BCELoss function\n",
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.MSELoss()\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# Setup Adam optimizers for both G and D\n",
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerDK = optim.Adam(netDK.parameters(), lr=lr, betas=(beta1, 0.999))\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "PATH = \"/PATH/TO/SAVED/MODEL/Gen-32-alph-1-epoch-99-complete.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "netG.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizerG.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "epoch_s = checkpoint['epoch']\n",
    "loss = checkpoint['loss']\n",
    "\n",
    "netG.eval()\n",
    "\n",
    "PATH = \"/PATH/TO/SAVED/MODEL/Dis-32-alph-1-epoch-99-complete.pt\"\n",
    "checkpoint = torch.load(PATH)\n",
    "netD.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizerD.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "optimizerDK.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# optimizerDK = copy.deepcopy(optimizerD)\n",
    "epoch_s1 = checkpoint['epoch']\n",
    "# loss = checkpoint['loss']\n",
    "# netDK = copy.deepcopy(netD)\n",
    "netDK.load_state_dict(netD.state_dict())\n",
    "\n",
    "netD.eval()\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "fixed_noise, _ = gen_fake(64) #torch.randn(64, nz+nb_digits, 1, 1, device=device)\n",
    "real_batch = next(iter(dataloader))\n",
    "b_size = real_batch[0].size(0)\n",
    "noise, _ = gen_fake(batch_size) # torch.rand(b_size, nz+nb_digits, 1, 1, device=device)\n",
    "\n",
    "# flags = [[4, \"stable\"], [8, \"transition\"], [8, \"stable\"], [16, \"transition\"], [16, \"stable\"], [32, \"transition\"], [32, \"stable\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3, 64, 64])\n",
      "torch.Size([16, 1])\n",
      "y_onehot_fake:  torch.Size([16, 5])  y_onehot_real:  torch.Size([16, 5])\n",
      "torch.Size([16, 3, 64, 64])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 1])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GEN AND DISC CHECK\n",
    "flag = [64, \"transition\"]\n",
    "alpha = 0.1\n",
    "for i, data in enumerate(dataloader, 0):\n",
    "    if i==0:\n",
    "        break\n",
    "print(data[0].to(device).shape)\n",
    "print(netD(data[0].to(device),turn_label_to_one_hot(data[1].to(device)), flag = flag, alpha=alpha).shape)\n",
    "z_fake, y_onehot_fake = gen_fake(batch_size)\n",
    "print(\"y_onehot_fake: \", y_onehot_fake.shape, \" y_onehot_real: \", turn_label_to_one_hot(data[1].to(device)).shape)\n",
    "print(netG(z_fake, flag = flag, alpha=alpha).shape)\n",
    "netD(netG(z_fake, flag = flag, alpha=alpha), y_onehot_fake, flag = flag, alpha=alpha).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Training Loop...\n",
      "[0/131][0/381]\tLoss_D: -236224432.0000\tLoss_G: 90336208.0000\tD(x): 145625120.0000\tD(G(z)): -90599312.0000 / -90336208.0000 \t alpha: 0.0000\n",
      "[0/131][50/381]\tLoss_D: -52932.0000\tLoss_G: -44678200.0000\tD(x): 44795244.0000\tD(G(z)): 44742312.0000 / 44678200.0000 \t alpha: 0.0010\n",
      "[0/131][100/381]\tLoss_D: 20004.0000\tLoss_G: -10873172.0000\tD(x): 11060832.0000\tD(G(z)): 11080836.0000 / 10873172.0000 \t alpha: 0.0020\n",
      "[0/131][150/381]\tLoss_D: -101572656.0000\tLoss_G: -10973203.0000\tD(x): 112346416.0000\tD(G(z)): 10773759.0000 / 10973203.0000 \t alpha: 0.0030\n",
      "[0/131][200/381]\tLoss_D: -34010900.0000\tLoss_G: 22974744.0000\tD(x): 11009969.0000\tD(G(z)): -23000932.0000 / -22974744.0000 \t alpha: 0.0040\n",
      "[0/131][250/381]\tLoss_D: -101674064.0000\tLoss_G: 56691408.0000\tD(x): 44975460.0000\tD(G(z)): -56698608.0000 / -56691408.0000 \t alpha: 0.0050\n",
      "[0/131][300/381]\tLoss_D: -33928396.0000\tLoss_G: 25595376.0000\tD(x): 8912856.0000\tD(G(z)): -25015540.0000 / -25595376.0000 \t alpha: 0.0060\n",
      "[0/131][350/381]\tLoss_D: -101443912.0000\tLoss_G: 56641340.0000\tD(x): 44821512.0000\tD(G(z)): -56622400.0000 / -56641340.0000 \t alpha: 0.0071\n",
      "[1/131][0/381]\tLoss_D: -135541232.0000\tLoss_G: 90603808.0000\tD(x): 45006180.0000\tD(G(z)): -90535056.0000 / -90603808.0000 \t alpha: 0.0077\n",
      "[1/131][50/381]\tLoss_D: -33926464.0000\tLoss_G: -10958268.0000\tD(x): 44986220.0000\tD(G(z)): 11059756.0000 / 10958268.0000 \t alpha: 0.0087\n",
      "[1/131][100/381]\tLoss_D: 33861988.0000\tLoss_G: -10926346.0000\tD(x): -22743492.0000\tD(G(z)): 11118495.0000 / 10926346.0000 \t alpha: 0.0097\n",
      "[1/131][150/381]\tLoss_D: -101917224.0000\tLoss_G: 90782560.0000\tD(x): 11112944.0000\tD(G(z)): -90804280.0000 / -90782560.0000 \t alpha: 0.0107\n",
      "[1/131][200/381]\tLoss_D: -102140304.0000\tLoss_G: -10942028.0000\tD(x): 113204456.0000\tD(G(z)): 11064152.0000 / 10942028.0000 \t alpha: 0.0117\n",
      "[1/131][250/381]\tLoss_D: -68131032.0000\tLoss_G: 22973776.0000\tD(x): 45210776.0000\tD(G(z)): -22920256.0000 / -22973776.0000 \t alpha: 0.0127\n",
      "[1/131][300/381]\tLoss_D: -34097424.0000\tLoss_G: 57070464.0000\tD(x): -22854496.0000\tD(G(z)): -56951920.0000 / -57070464.0000 \t alpha: 0.0137\n",
      "[1/131][350/381]\tLoss_D: -68129232.0000\tLoss_G: 22922628.0000\tD(x): 45137560.0000\tD(G(z)): -22991676.0000 / -22922628.0000 \t alpha: 0.0147\n",
      "[2/131][0/381]\tLoss_D: 68180784.0000\tLoss_G: -45202324.0000\tD(x): -22893896.0000\tD(G(z)): 45286888.0000 / 45202324.0000 \t alpha: 0.0153\n",
      "[2/131][50/381]\tLoss_D: -102369376.0000\tLoss_G: 57013684.0000\tD(x): 45360648.0000\tD(G(z)): -57008732.0000 / -57013684.0000 \t alpha: 0.0163\n",
      "[2/131][100/381]\tLoss_D: -307226496.0000\tLoss_G: 193633344.0000\tD(x): 113619968.0000\tD(G(z)): -193606544.0000 / -193633344.0000 \t alpha: 0.0173\n",
      "[2/131][150/381]\tLoss_D: -102508520.0000\tLoss_G: 57242960.0000\tD(x): 45237488.0000\tD(G(z)): -57271032.0000 / -57242960.0000 \t alpha: 0.0183\n",
      "[2/131][200/381]\tLoss_D: -136770928.0000\tLoss_G: 91412992.0000\tD(x): 45334984.0000\tD(G(z)): -91435944.0000 / -91412992.0000 \t alpha: 0.0193\n",
      "[2/131][250/381]\tLoss_D: -136775264.0000\tLoss_G: 57167312.0000\tD(x): 79591816.0000\tD(G(z)): -57183448.0000 / -57167312.0000 \t alpha: 0.0203\n",
      "[2/131][300/381]\tLoss_D: -68506456.0000\tLoss_G: -45395708.0000\tD(x): 113853072.0000\tD(G(z)): 45346616.0000 / 45395708.0000 \t alpha: 0.0214\n",
      "[2/131][350/381]\tLoss_D: -10512.0000\tLoss_G: -113882864.0000\tD(x): 113979056.0000\tD(G(z)): 113968544.0000 / 113882864.0000 \t alpha: 0.0224\n",
      "[3/131][0/381]\tLoss_D: 136852672.0000\tLoss_G: -44302080.0000\tD(x): -91557608.0000\tD(G(z)): 45295060.0000 / 44302080.0000 \t alpha: 0.0230\n",
      "[3/131][50/381]\tLoss_D: -68591744.0000\tLoss_G: 57287480.0000\tD(x): 11316873.0000\tD(G(z)): -57274872.0000 / -57287480.0000 \t alpha: 0.0240\n",
      "[3/131][100/381]\tLoss_D: -68645048.0000\tLoss_G: 57514176.0000\tD(x): 11259940.0000\tD(G(z)): -57385108.0000 / -57514176.0000 \t alpha: 0.0250\n",
      "[3/131][150/381]\tLoss_D: -171500368.0000\tLoss_G: 57267916.0000\tD(x): 114150208.0000\tD(G(z)): -57350160.0000 / -57267916.0000 \t alpha: 0.0260\n",
      "[3/131][200/381]\tLoss_D: -171774560.0000\tLoss_G: 57417572.0000\tD(x): 114362624.0000\tD(G(z)): -57411928.0000 / -57417572.0000 \t alpha: 0.0270\n",
      "[3/131][250/381]\tLoss_D: -103077040.0000\tLoss_G: 22992536.0000\tD(x): 80070368.0000\tD(G(z)): -23006676.0000 / -22992536.0000 \t alpha: 0.0280\n",
      "[3/131][300/381]\tLoss_D: -171953312.0000\tLoss_G: 57432516.0000\tD(x): 114513360.0000\tD(G(z)): -57439956.0000 / -57432516.0000 \t alpha: 0.0290\n",
      "[3/131][350/381]\tLoss_D: -52868.0000\tLoss_G: -11293124.0000\tD(x): 11372500.0000\tD(G(z)): 11319632.0000 / 11293124.0000 \t alpha: 0.0300\n",
      "[4/131][0/381]\tLoss_D: 68626640.0000\tLoss_G: -78833632.0000\tD(x): 11184626.0000\tD(G(z)): 79811264.0000 / 78833632.0000 \t alpha: 0.0306\n",
      "[4/131][50/381]\tLoss_D: -34472296.0000\tLoss_G: 57517824.0000\tD(x): -23004584.0000\tD(G(z)): -57476880.0000 / -57517824.0000 \t alpha: 0.0316\n",
      "[4/131][100/381]\tLoss_D: -30824.0000\tLoss_G: -80287760.0000\tD(x): 80260064.0000\tD(G(z)): 80229240.0000 / 80287760.0000 \t alpha: 0.0326\n",
      "[4/131][150/381]\tLoss_D: -34450360.0000\tLoss_G: 92007512.0000\tD(x): -57517032.0000\tD(G(z)): -91967392.0000 / -92007512.0000 \t alpha: 0.0336\n",
      "[4/131][200/381]\tLoss_D: -68979824.0000\tLoss_G: 23084548.0000\tD(x): 45946568.0000\tD(G(z)): -23033258.0000 / -23084548.0000 \t alpha: 0.0347\n",
      "[4/131][250/381]\tLoss_D: -138129248.0000\tLoss_G: 57570428.0000\tD(x): 80568616.0000\tD(G(z)): -57560624.0000 / -57570428.0000 \t alpha: 0.0357\n",
      "[4/131][300/381]\tLoss_D: -207224064.0000\tLoss_G: 195725280.0000\tD(x): 11514627.0000\tD(G(z)): -195709440.0000 / -195725280.0000 \t alpha: 0.0367\n",
      "[4/131][350/381]\tLoss_D: -138273568.0000\tLoss_G: 23080608.0000\tD(x): 115198256.0000\tD(G(z)): -23075312.0000 / -23080608.0000 \t alpha: 0.0377\n",
      "[5/131][0/381]\tLoss_D: -69191432.0000\tLoss_G: -11493599.0000\tD(x): 80665424.0000\tD(G(z)): 11473994.0000 / 11493599.0000 \t alpha: 0.0383\n",
      "[5/131][50/381]\tLoss_D: -69308344.0000\tLoss_G: 92892312.0000\tD(x): -23165160.0000\tD(G(z)): -92473504.0000 / -92892312.0000 \t alpha: 0.0393\n",
      "[5/131][100/381]\tLoss_D: -69291488.0000\tLoss_G: -11528276.0000\tD(x): 80820544.0000\tD(G(z)): 11529056.0000 / 11528276.0000 \t alpha: 0.0403\n",
      "[5/131][150/381]\tLoss_D: -173160832.0000\tLoss_G: 23066972.0000\tD(x): 150045600.0000\tD(G(z)): -23115232.0000 / -23066972.0000 \t alpha: 0.0413\n",
      "[5/131][200/381]\tLoss_D: -138649600.0000\tLoss_G: 92447032.0000\tD(x): 46219488.0000\tD(G(z)): -92430104.0000 / -92447032.0000 \t alpha: 0.0423\n",
      "[5/131][250/381]\tLoss_D: -69410752.0000\tLoss_G: -11557576.0000\tD(x): 80971584.0000\tD(G(z)): 11560828.0000 / 11557576.0000 \t alpha: 0.0433\n",
      "[5/131][300/381]\tLoss_D: 34682692.0000\tLoss_G: -11518100.0000\tD(x): -23140568.0000\tD(G(z)): 11542123.0000 / 11518100.0000 \t alpha: 0.0443\n",
      "[5/131][350/381]\tLoss_D: -173697728.0000\tLoss_G: 196852976.0000\tD(x): -23131048.0000\tD(G(z)): -196828784.0000 / -196852976.0000 \t alpha: 0.0453\n",
      "[6/131][0/381]\tLoss_D: -104194560.0000\tLoss_G: 92648824.0000\tD(x): 11575284.0000\tD(G(z)): -92619280.0000 / -92648824.0000 \t alpha: 0.0459\n",
      "[6/131][50/381]\tLoss_D: 34760960.0000\tLoss_G: 23688512.0000\tD(x): -57939976.0000\tD(G(z)): -23179016.0000 / -23688512.0000 \t alpha: 0.0469\n",
      "[6/131][100/381]\tLoss_D: 34799564.0000\tLoss_G: -81187160.0000\tD(x): 46370420.0000\tD(G(z)): 81169984.0000 / 81187160.0000 \t alpha: 0.0480\n",
      "[6/131][150/381]\tLoss_D: -104432040.0000\tLoss_G: 23137116.0000\tD(x): 81308256.0000\tD(G(z)): -23123784.0000 / -23137116.0000 \t alpha: 0.0490\n",
      "[6/131][200/381]\tLoss_D: -243771840.0000\tLoss_G: 127724920.0000\tD(x): 116025120.0000\tD(G(z)): -127746720.0000 / -127724920.0000 \t alpha: 0.0500\n",
      "[6/131][250/381]\tLoss_D: -5976.0000\tLoss_G: 23273648.0000\tD(x): -23219068.0000\tD(G(z)): -23225044.0000 / -23273648.0000 \t alpha: 0.0510\n",
      "[6/131][300/381]\tLoss_D: -104709184.0000\tLoss_G: 23229868.0000\tD(x): 81454736.0000\tD(G(z)): -23254444.0000 / -23229868.0000 \t alpha: 0.0520\n",
      "[6/131][350/381]\tLoss_D: 34863592.0000\tLoss_G: 23397760.0000\tD(x): -58131076.0000\tD(G(z)): -23267484.0000 / -23397760.0000 \t alpha: 0.0530\n",
      "[7/131][0/381]\tLoss_D: -69883328.0000\tLoss_G: 23511592.0000\tD(x): 46414536.0000\tD(G(z)): -23468788.0000 / -23511592.0000 \t alpha: 0.0536\n",
      "[7/131][50/381]\tLoss_D: -139820672.0000\tLoss_G: 58232456.0000\tD(x): 81576552.0000\tD(G(z)): -58244124.0000 / -58232456.0000 \t alpha: 0.0546\n",
      "[7/131][100/381]\tLoss_D: 34889184.0000\tLoss_G: -46648512.0000\tD(x): 11786959.0000\tD(G(z)): 46676144.0000 / 46648512.0000 \t alpha: 0.0556\n",
      "[7/131][150/381]\tLoss_D: -174835328.0000\tLoss_G: 128126536.0000\tD(x): 46715388.0000\tD(G(z)): -128119944.0000 / -128126536.0000 \t alpha: 0.0566\n",
      "[7/131][200/381]\tLoss_D: 104872208.0000\tLoss_G: -81691184.0000\tD(x): -23169980.0000\tD(G(z)): 81702224.0000 / 81691184.0000 \t alpha: 0.0576\n",
      "[7/131][250/381]\tLoss_D: -105059504.0000\tLoss_G: 58348204.0000\tD(x): 46776428.0000\tD(G(z)): -58283080.0000 / -58348204.0000 \t alpha: 0.0586\n",
      "[7/131][300/381]\tLoss_D: -105164696.0000\tLoss_G: 93406752.0000\tD(x): 11783942.0000\tD(G(z)): -93380752.0000 / -93406752.0000 \t alpha: 0.0596\n",
      "[7/131][350/381]\tLoss_D: -49376.0000\tLoss_G: 93844280.0000\tD(x): -93313736.0000\tD(G(z)): -93363112.0000 / -93844280.0000 \t alpha: 0.0606\n",
      "[8/131][0/381]\tLoss_D: -70172464.0000\tLoss_G: 58750852.0000\tD(x): 11774236.0000\tD(G(z)): -58398232.0000 / -58750852.0000 \t alpha: 0.0612\n",
      "[8/131][50/381]\tLoss_D: -70169080.0000\tLoss_G: -46879392.0000\tD(x): 117006336.0000\tD(G(z)): 46837256.0000 / 46879392.0000 \t alpha: 0.0623\n",
      "[8/131][100/381]\tLoss_D: -23864.0000\tLoss_G: -81981168.0000\tD(x): 82024232.0000\tD(G(z)): 82000368.0000 / 81981168.0000 \t alpha: 0.0633\n",
      "[8/131][150/381]\tLoss_D: -105351400.0000\tLoss_G: 58358148.0000\tD(x): 47003560.0000\tD(G(z)): -58347840.0000 / -58358148.0000 \t alpha: 0.0643\n",
      "[8/131][200/381]\tLoss_D: -35130976.0000\tLoss_G: 23326220.0000\tD(x): 11850076.0000\tD(G(z)): -23280902.0000 / -23326220.0000 \t alpha: 0.0653\n",
      "[8/131][250/381]\tLoss_D: -69775792.0000\tLoss_G: 95232952.0000\tD(x): -24472002.0000\tD(G(z)): -94247792.0000 / -95232952.0000 \t alpha: 0.0663\n",
      "[8/131][300/381]\tLoss_D: -246422480.0000\tLoss_G: 199011952.0000\tD(x): 47122140.0000\tD(G(z)): -199300336.0000 / -199011952.0000 \t alpha: 0.0673\n",
      "[8/131][350/381]\tLoss_D: -145404.0000\tLoss_G: 23686940.0000\tD(x): -23355278.0000\tD(G(z)): -23500682.0000 / -23686940.0000 \t alpha: 0.0683\n",
      "[9/131][0/381]\tLoss_D: -140699968.0000\tLoss_G: 23381334.0000\tD(x): 117270592.0000\tD(G(z)): -23429368.0000 / -23381334.0000 \t alpha: 0.0689\n",
      "[9/131][50/381]\tLoss_D: -176461280.0000\tLoss_G: 95195952.0000\tD(x): 81777704.0000\tD(G(z)): -94683576.0000 / -95195952.0000 \t alpha: 0.0699\n",
      "[9/131][100/381]\tLoss_D: -246592256.0000\tLoss_G: 199764208.0000\tD(x): 46732456.0000\tD(G(z)): -199859808.0000 / -199764208.0000 \t alpha: 0.0709\n",
      "[9/131][150/381]\tLoss_D: 70530520.0000\tLoss_G: 23997308.0000\tD(x): -94046824.0000\tD(G(z)): -23516304.0000 / -23997308.0000 \t alpha: 0.0719\n",
      "[9/131][200/381]\tLoss_D: -106233024.0000\tLoss_G: 129302064.0000\tD(x): -23472892.0000\tD(G(z)): -129705920.0000 / -129302064.0000 \t alpha: 0.0729\n",
      "[9/131][250/381]\tLoss_D: -70447656.0000\tLoss_G: -47126088.0000\tD(x): 117383472.0000\tD(G(z)): 46935816.0000 / 47126088.0000 \t alpha: 0.0739\n",
      "[9/131][300/381]\tLoss_D: -106094984.0000\tLoss_G: 94175080.0000\tD(x): 11889407.0000\tD(G(z)): -94205576.0000 / -94175080.0000 \t alpha: 0.0749\n",
      "[9/131][350/381]\tLoss_D: -70672800.0000\tLoss_G: 58913864.0000\tD(x): 11381294.0000\tD(G(z)): -59291508.0000 / -58913864.0000 \t alpha: 0.0759\n",
      "[10/131][0/381]\tLoss_D: -70740560.0000\tLoss_G: 62041796.0000\tD(x): 12060446.0000\tD(G(z)): -58680112.0000 / -62041796.0000 \t alpha: 0.0766\n",
      "[10/131][50/381]\tLoss_D: -141338816.0000\tLoss_G: 59094880.0000\tD(x): 82082912.0000\tD(G(z)): -59255912.0000 / -59094880.0000 \t alpha: 0.0776\n",
      "[10/131][100/381]\tLoss_D: -142112608.0000\tLoss_G: 94432736.0000\tD(x): 47766456.0000\tD(G(z)): -94346160.0000 / -94432736.0000 \t alpha: 0.0786\n",
      "[10/131][150/381]\tLoss_D: -248098560.0000\tLoss_G: 236252544.0000\tD(x): 11940636.0000\tD(G(z)): -236157920.0000 / -236252544.0000 \t alpha: 0.0796\n",
      "[10/131][200/381]\tLoss_D: -248054752.0000\tLoss_G: 165654544.0000\tD(x): 82433072.0000\tD(G(z)): -165621680.0000 / -165654544.0000 \t alpha: 0.0806\n",
      "[10/131][250/381]\tLoss_D: 35219656.0000\tLoss_G: -47328680.0000\tD(x): 12163562.0000\tD(G(z)): 47383220.0000 / 47328680.0000 \t alpha: 0.0816\n",
      "[10/131][300/381]\tLoss_D: -35625020.0000\tLoss_G: -46883528.0000\tD(x): 83057488.0000\tD(G(z)): 47432468.0000 / 46883528.0000 \t alpha: 0.0826\n",
      "[10/131][350/381]\tLoss_D: -35782772.0000\tLoss_G: 27014916.0000\tD(x): 11966671.0000\tD(G(z)): -23816100.0000 / -27014916.0000 \t alpha: 0.0836\n",
      "[11/131][0/381]\tLoss_D: -178297280.0000\tLoss_G: 94942848.0000\tD(x): 83319104.0000\tD(G(z)): -94978168.0000 / -94942848.0000 \t alpha: 0.0842\n",
      "[11/131][50/381]\tLoss_D: -106681616.0000\tLoss_G: 130240544.0000\tD(x): -23653524.0000\tD(G(z)): -130335144.0000 / -130240544.0000 \t alpha: 0.0852\n",
      "[11/131][100/381]\tLoss_D: -71176280.0000\tLoss_G: 94910720.0000\tD(x): -23786106.0000\tD(G(z)): -94962384.0000 / -94910720.0000 \t alpha: 0.0862\n",
      "[11/131][150/381]\tLoss_D: -35695136.0000\tLoss_G: 59466260.0000\tD(x): -23615878.0000\tD(G(z)): -59311016.0000 / -59466260.0000 \t alpha: 0.0872\n",
      "[11/131][200/381]\tLoss_D: 35504824.0000\tLoss_G: 95408176.0000\tD(x): -130439160.0000\tD(G(z)): -94934336.0000 / -95408176.0000 \t alpha: 0.0882\n",
      "[11/131][250/381]\tLoss_D: -35752500.0000\tLoss_G: 23749648.0000\tD(x): 12152340.0000\tD(G(z)): -23600160.0000 / -23749648.0000 \t alpha: 0.0892\n",
      "[11/131][300/381]\tLoss_D: -107187072.0000\tLoss_G: 23735494.0000\tD(x): 83552632.0000\tD(G(z)): -23634442.0000 / -23735494.0000 \t alpha: 0.0902\n",
      "[11/131][350/381]\tLoss_D: -35709188.0000\tLoss_G: 59487180.0000\tD(x): -23682344.0000\tD(G(z)): -59391532.0000 / -59487180.0000 \t alpha: 0.0912\n",
      "[12/131][0/381]\tLoss_D: -178651040.0000\tLoss_G: 59314344.0000\tD(x): 119311464.0000\tD(G(z)): -59339572.0000 / -59314344.0000 \t alpha: 0.0919\n",
      "[12/131][50/381]\tLoss_D: -285993728.0000\tLoss_G: 130815720.0000\tD(x): 155192432.0000\tD(G(z)): -130801288.0000 / -130815720.0000 \t alpha: 0.0929\n",
      "[12/131][100/381]\tLoss_D: -178894176.0000\tLoss_G: 23773848.0000\tD(x): 155184224.0000\tD(G(z)): -23709960.0000 / -23773848.0000 \t alpha: 0.0939\n",
      "[12/131][150/381]\tLoss_D: 35761892.0000\tLoss_G: -10008902.0000\tD(x): -23621402.0000\tD(G(z)): 12140490.0000 / 10008902.0000 \t alpha: 0.0949\n",
      "[12/131][200/381]\tLoss_D: -107427824.0000\tLoss_G: 23681896.0000\tD(x): 83802736.0000\tD(G(z)): -23625088.0000 / -23681896.0000 \t alpha: 0.0959\n",
      "[12/131][250/381]\tLoss_D: -214990064.0000\tLoss_G: 131213904.0000\tD(x): 83804728.0000\tD(G(z)): -131185336.0000 / -131213904.0000 \t alpha: 0.0969\n",
      "[12/131][300/381]\tLoss_D: -71701888.0000\tLoss_G: -12196329.0000\tD(x): 83854064.0000\tD(G(z)): 12152173.0000 / 12196329.0000 \t alpha: 0.0979\n",
      "[12/131][350/381]\tLoss_D: -35861964.0000\tLoss_G: -10979274.0000\tD(x): 47860900.0000\tD(G(z)): 11998935.0000 / 10979274.0000 \t alpha: 0.0989\n",
      "[13/131][0/381]\tLoss_D: -35804648.0000\tLoss_G: -11651210.0000\tD(x): 47497620.0000\tD(G(z)): 11692972.0000 / 11651210.0000 \t alpha: 0.0995\n",
      "[13/131][50/381]\tLoss_D: -129896.0000\tLoss_G: -11236658.0000\tD(x): 12087294.0000\tD(G(z)): 11957398.0000 / 11236658.0000 \t alpha: 0.1005\n",
      "[13/131][100/381]\tLoss_D: -179724816.0000\tLoss_G: 59698912.0000\tD(x): 120012440.0000\tD(G(z)): -59712372.0000 / -59698912.0000 \t alpha: 0.1015\n",
      "[13/131][150/381]\tLoss_D: -107806016.0000\tLoss_G: -12217026.0000\tD(x): 120043336.0000\tD(G(z)): 12237316.0000 / 12217026.0000 \t alpha: 0.1025\n",
      "[13/131][200/381]\tLoss_D: 107790656.0000\tLoss_G: -46858436.0000\tD(x): -59636824.0000\tD(G(z)): 48153832.0000 / 46858436.0000 \t alpha: 0.1035\n",
      "[13/131][250/381]\tLoss_D: -108006160.0000\tLoss_G: 59882764.0000\tD(x): 48161688.0000\tD(G(z)): -59844472.0000 / -59882764.0000 \t alpha: 0.1045\n",
      "[13/131][300/381]\tLoss_D: -144013680.0000\tLoss_G: 59705572.0000\tD(x): 84313040.0000\tD(G(z)): -59700640.0000 / -59705572.0000 \t alpha: 0.1055\n",
      "[13/131][350/381]\tLoss_D: -216147552.0000\tLoss_G: 167853744.0000\tD(x): 48298212.0000\tD(G(z)): -167849344.0000 / -167853744.0000 \t alpha: 0.1065\n",
      "[14/131][0/381]\tLoss_D: -36072368.0000\tLoss_G: -48277388.0000\tD(x): 84438296.0000\tD(G(z)): 48365928.0000 / 48277388.0000 \t alpha: 0.1072\n",
      "[14/131][50/381]\tLoss_D: -180296480.0000\tLoss_G: 95833440.0000\tD(x): 84487632.0000\tD(G(z)): -95808848.0000 / -95833440.0000 \t alpha: 0.1082\n",
      "[14/131][100/381]\tLoss_D: -108273040.0000\tLoss_G: 168275392.0000\tD(x): -59817228.0000\tD(G(z)): -168090272.0000 / -168275392.0000 \t alpha: 0.1092\n",
      "[14/131][150/381]\tLoss_D: -72291904.0000\tLoss_G: 96274680.0000\tD(x): -23750784.0000\tD(G(z)): -96042688.0000 / -96274680.0000 \t alpha: 0.1102\n",
      "[14/131][200/381]\tLoss_D: -180688256.0000\tLoss_G: 23720984.0000\tD(x): 156925408.0000\tD(G(z)): -23762856.0000 / -23720984.0000 \t alpha: 0.1112\n",
      "[14/131][250/381]\tLoss_D: 105150680.0000\tLoss_G: 11429877.0000\tD(x): -60540264.0000\tD(G(z)): 44610416.0000 / -11429877.0000 \t alpha: 0.1122\n",
      "[14/131][300/381]\tLoss_D: 71609072.0000\tLoss_G: -117166608.0000\tD(x): 47786556.0000\tD(G(z)): 119395632.0000 / 117166608.0000 \t alpha: 0.1132\n",
      "[14/131][350/381]\tLoss_D: -88640.0000\tLoss_G: -120648312.0000\tD(x): 120521472.0000\tD(G(z)): 120432832.0000 / 120648312.0000 \t alpha: 0.1142\n",
      "[15/131][0/381]\tLoss_D: -72636968.0000\tLoss_G: -11073981.0000\tD(x): 82663336.0000\tD(G(z)): 10026365.0000 / 11073981.0000 \t alpha: 0.1148\n",
      "[15/131][50/381]\tLoss_D: -108467440.0000\tLoss_G: 23957944.0000\tD(x): 84514656.0000\tD(G(z)): -23952788.0000 / -23957944.0000 \t alpha: 0.1158\n",
      "[15/131][100/381]\tLoss_D: -72575840.0000\tLoss_G: 24000672.0000\tD(x): 48477864.0000\tD(G(z)): -24097972.0000 / -24000672.0000 \t alpha: 0.1168\n",
      "[15/131][150/381]\tLoss_D: -36273672.0000\tLoss_G: -12344540.0000\tD(x): 48568528.0000\tD(G(z)): 12294856.0000 / 12344540.0000 \t alpha: 0.1178\n",
      "[15/131][200/381]\tLoss_D: -108845616.0000\tLoss_G: -48378960.0000\tD(x): 157193376.0000\tD(G(z)): 48347756.0000 / 48378960.0000 \t alpha: 0.1188\n",
      "[15/131][250/381]\tLoss_D: -108650456.0000\tLoss_G: 132844688.0000\tD(x): -24413800.0000\tD(G(z)): -133064256.0000 / -132844688.0000 \t alpha: 0.1198\n",
      "[15/131][300/381]\tLoss_D: 72367560.0000\tLoss_G: -10617131.0000\tD(x): -60334456.0000\tD(G(z)): 12033107.0000 / 10617131.0000 \t alpha: 0.1209\n",
      "[15/131][350/381]\tLoss_D: -387904.0000\tLoss_G: -84282800.0000\tD(x): 84551856.0000\tD(G(z)): 84163952.0000 / 84282800.0000 \t alpha: 0.1219\n",
      "[16/131][0/381]\tLoss_D: -3848.0000\tLoss_G: -11493404.0000\tD(x): 11559456.0000\tD(G(z)): 11555608.0000 / 11493404.0000 \t alpha: 0.1225\n",
      "[16/131][50/381]\tLoss_D: -72694.0000\tLoss_G: -12305150.0000\tD(x): 12458440.0000\tD(G(z)): 12385746.0000 / 12305150.0000 \t alpha: 0.1235\n",
      "[16/131][100/381]\tLoss_D: -72747696.0000\tLoss_G: 96701368.0000\tD(x): -23922248.0000\tD(G(z)): -96669944.0000 / -96701368.0000 \t alpha: 0.1245\n",
      "[16/131][150/381]\tLoss_D: 36375168.0000\tLoss_G: -12176326.0000\tD(x): -23964572.0000\tD(G(z)): 12410596.0000 / 12176326.0000 \t alpha: 0.1255\n",
      "[16/131][200/381]\tLoss_D: -109365984.0000\tLoss_G: 60659584.0000\tD(x): 48932492.0000\tD(G(z)): -60433488.0000 / -60659584.0000 \t alpha: 0.1265\n",
      "[16/131][250/381]\tLoss_D: -36571520.0000\tLoss_G: -11785740.0000\tD(x): 48729088.0000\tD(G(z)): 12157568.0000 / 11785740.0000 \t alpha: 0.1275\n",
      "[16/131][300/381]\tLoss_D: 73028224.0000\tLoss_G: 25770270.0000\tD(x): -97066384.0000\tD(G(z)): -24038160.0000 / -25770270.0000 \t alpha: 0.1285\n",
      "[16/131][350/381]\tLoss_D: -109552048.0000\tLoss_G: 24056520.0000\tD(x): 85533816.0000\tD(G(z)): -24018232.0000 / -24056520.0000 \t alpha: 0.1295\n",
      "[17/131][0/381]\tLoss_D: -36469808.0000\tLoss_G: 60503116.0000\tD(x): -23980544.0000\tD(G(z)): -60450352.0000 / -60503116.0000 \t alpha: 0.1301\n",
      "[17/131][50/381]\tLoss_D: -68628.0000\tLoss_G: 61180084.0000\tD(x): -60487624.0000\tD(G(z)): -60556252.0000 / -61180084.0000 \t alpha: 0.1311\n",
      "[17/131][100/381]\tLoss_D: -65268.0000\tLoss_G: -48973148.0000\tD(x): 49185988.0000\tD(G(z)): 49120720.0000 / 48973148.0000 \t alpha: 0.1321\n",
      "[17/131][150/381]\tLoss_D: -148627008.0000\tLoss_G: 94249896.0000\tD(x): 84131056.0000\tD(G(z)): -64495952.0000 / -94249896.0000 \t alpha: 0.1331\n",
      "[17/131][200/381]\tLoss_D: 182640512.0000\tLoss_G: -115985288.0000\tD(x): -60867924.0000\tD(G(z)): 121772592.0000 / 115985288.0000 \t alpha: 0.1342\n",
      "[17/131][250/381]\tLoss_D: -74015632.0000\tLoss_G: -9574006.0000\tD(x): 84910936.0000\tD(G(z)): 10895307.0000 / 9574006.0000 \t alpha: 0.1352\n",
      "[17/131][300/381]\tLoss_D: 123153480.0000\tLoss_G: 34298976.0000\tD(x): -221552304.0000\tD(G(z)): -98398824.0000 / -34298976.0000 \t alpha: 0.1362\n",
      "[17/131][350/381]\tLoss_D: 146359536.0000\tLoss_G: 65678200.0000\tD(x): -207358304.0000\tD(G(z)): -60998772.0000 / -65678200.0000 \t alpha: 0.1372\n",
      "[18/131][0/381]\tLoss_D: -146542720.0000\tLoss_G: 61457244.0000\tD(x): 84436152.0000\tD(G(z)): -62106576.0000 / -61457244.0000 \t alpha: 0.1378\n",
      "[18/131][50/381]\tLoss_D: 36684328.0000\tLoss_G: -49206420.0000\tD(x): 12670024.0000\tD(G(z)): 49354352.0000 / 49206420.0000 \t alpha: 0.1388\n",
      "[18/131][100/381]\tLoss_D: -146940352.0000\tLoss_G: 97636168.0000\tD(x): 49342592.0000\tD(G(z)): -97597752.0000 / -97636168.0000 \t alpha: 0.1398\n",
      "[18/131][150/381]\tLoss_D: -37021656.0000\tLoss_G: 99333048.0000\tD(x): -61278544.0000\tD(G(z)): -98300200.0000 / -99333048.0000 \t alpha: 0.1408\n",
      "[18/131][200/381]\tLoss_D: -73547672.0000\tLoss_G: 60908928.0000\tD(x): 12542998.0000\tD(G(z)): -61004672.0000 / -60908928.0000 \t alpha: 0.1418\n",
      "[18/131][250/381]\tLoss_D: -147029456.0000\tLoss_G: 60902888.0000\tD(x): 86177344.0000\tD(G(z)): -60852108.0000 / -60902888.0000 \t alpha: 0.1428\n",
      "[18/131][300/381]\tLoss_D: -106230.0000\tLoss_G: -12363420.0000\tD(x): 12713120.0000\tD(G(z)): 12606890.0000 / 12363420.0000 \t alpha: 0.1438\n",
      "[18/131][350/381]\tLoss_D: -184153280.0000\tLoss_G: 134606080.0000\tD(x): 49553892.0000\tD(G(z)): -134599392.0000 / -134606080.0000 \t alpha: 0.1448\n",
      "[19/131][0/381]\tLoss_D: -184061648.0000\tLoss_G: 98293112.0000\tD(x): 85834336.0000\tD(G(z)): -98227312.0000 / -98293112.0000 \t alpha: 0.1454\n",
      "[19/131][50/381]\tLoss_D: 15332.0000\tLoss_G: -49621584.0000\tD(x): 49673488.0000\tD(G(z)): 49688820.0000 / 49621584.0000 \t alpha: 0.1464\n",
      "[19/131][100/381]\tLoss_D: -37015824.0000\tLoss_G: -12269805.0000\tD(x): 49797268.0000\tD(G(z)): 12781442.0000 / 12269805.0000 \t alpha: 0.1474\n",
      "[19/131][150/381]\tLoss_D: -110789216.0000\tLoss_G: 98179936.0000\tD(x): 12734148.0000\tD(G(z)): -98055072.0000 / -98179936.0000 \t alpha: 0.1485\n",
      "[19/131][200/381]\tLoss_D: 37089692.0000\tLoss_G: -11136480.0000\tD(x): -24421482.0000\tD(G(z)): 12668211.0000 / 11136480.0000 \t alpha: 0.1495\n",
      "[19/131][250/381]\tLoss_D: -147932768.0000\tLoss_G: 98415296.0000\tD(x): 49797752.0000\tD(G(z)): -98135016.0000 / -98415296.0000 \t alpha: 0.1505\n",
      "[19/131][300/381]\tLoss_D: -111079016.0000\tLoss_G: 61380180.0000\tD(x): 49916712.0000\tD(G(z)): -61162304.0000 / -61380180.0000 \t alpha: 0.1515\n",
      "[19/131][350/381]\tLoss_D: -111112560.0000\tLoss_G: 24379710.0000\tD(x): 86961792.0000\tD(G(z)): -24150764.0000 / -24379710.0000 \t alpha: 0.1525\n",
      "[20/131][0/381]\tLoss_D: -111038416.0000\tLoss_G: 61250364.0000\tD(x): 49881556.0000\tD(G(z)): -61156864.0000 / -61250364.0000 \t alpha: 0.1531\n",
      "[20/131][50/381]\tLoss_D: 5689.0000\tLoss_G: -12885016.0000\tD(x): 12923485.0000\tD(G(z)): 12929174.0000 / 12885016.0000 \t alpha: 0.1541\n",
      "[20/131][100/381]\tLoss_D: -148342688.0000\tLoss_G: -49583164.0000\tD(x): 198087040.0000\tD(G(z)): 49744344.0000 / 49583164.0000 \t alpha: 0.1551\n",
      "[20/131][150/381]\tLoss_D: -37127348.0000\tLoss_G: 61269328.0000\tD(x): -24124632.0000\tD(G(z)): -61251980.0000 / -61269328.0000 \t alpha: 0.1561\n",
      "[20/131][200/381]\tLoss_D: -148482144.0000\tLoss_G: 61663188.0000\tD(x): 87146824.0000\tD(G(z)): -61335324.0000 / -61663188.0000 \t alpha: 0.1571\n",
      "[20/131][250/381]\tLoss_D: -111406256.0000\tLoss_G: 24391924.0000\tD(x): 87163584.0000\tD(G(z)): -24242676.0000 / -24391924.0000 \t alpha: 0.1581\n",
      "[20/131][300/381]\tLoss_D: -260118592.0000\tLoss_G: 210048224.0000\tD(x): 50081024.0000\tD(G(z)): -210037568.0000 / -210048224.0000 \t alpha: 0.1591\n",
      "[20/131][350/381]\tLoss_D: -185899488.0000\tLoss_G: 135792592.0000\tD(x): 50122988.0000\tD(G(z)): -135776496.0000 / -135792592.0000 \t alpha: 0.1601\n",
      "[21/131][0/381]\tLoss_D: -111637920.0000\tLoss_G: 98846640.0000\tD(x): 12978382.0000\tD(G(z)): -98659536.0000 / -98846640.0000 \t alpha: 0.1607\n",
      "[21/131][50/381]\tLoss_D: -260450560.0000\tLoss_G: 98636232.0000\tD(x): 161837312.0000\tD(G(z)): -98613248.0000 / -98636232.0000 \t alpha: 0.1618\n",
      "[21/131][100/381]\tLoss_D: -74461576.0000\tLoss_G: 61444400.0000\tD(x): 13029621.0000\tD(G(z)): -61431952.0000 / -61444400.0000 \t alpha: 0.1628\n",
      "[21/131][150/381]\tLoss_D: -149058176.0000\tLoss_G: 61615636.0000\tD(x): 87489848.0000\tD(G(z)): -61568324.0000 / -61615636.0000 \t alpha: 0.1638\n",
      "[21/131][200/381]\tLoss_D: 39236.0000\tLoss_G: 25077448.0000\tD(x): -24300296.0000\tD(G(z)): -24261060.0000 / -25077448.0000 \t alpha: 0.1648\n",
      "[21/131][250/381]\tLoss_D: -111855600.0000\tLoss_G: 24220480.0000\tD(x): 87611768.0000\tD(G(z)): -24243828.0000 / -24220480.0000 \t alpha: 0.1658\n",
      "[21/131][300/381]\tLoss_D: -74636376.0000\tLoss_G: 61512808.0000\tD(x): 13112209.0000\tD(G(z)): -61524164.0000 / -61512808.0000 \t alpha: 0.1668\n",
      "[21/131][350/381]\tLoss_D: -149423808.0000\tLoss_G: 136559216.0000\tD(x): 13124744.0000\tD(G(z)): -136299056.0000 / -136559216.0000 \t alpha: 0.1678\n",
      "[22/131][0/381]\tLoss_D: -74718944.0000\tLoss_G: 98962184.0000\tD(x): -24227188.0000\tD(G(z)): -98946128.0000 / -98962184.0000 \t alpha: 0.1684\n",
      "[22/131][50/381]\tLoss_D: -224159088.0000\tLoss_G: 173790848.0000\tD(x): 50353344.0000\tD(G(z)): -173805744.0000 / -173790848.0000 \t alpha: 0.1694\n",
      "[22/131][100/381]\tLoss_D: 37287320.0000\tLoss_G: -12762276.0000\tD(x): -24234576.0000\tD(G(z)): 13052744.0000 / 12762276.0000 \t alpha: 0.1704\n",
      "[22/131][150/381]\tLoss_D: 37364544.0000\tLoss_G: -50059552.0000\tD(x): 12979662.0000\tD(G(z)): 50344208.0000 / 50059552.0000 \t alpha: 0.1714\n",
      "[22/131][200/381]\tLoss_D: -33040.0000\tLoss_G: -87772608.0000\tD(x): 88093712.0000\tD(G(z)): 88060672.0000 / 87772608.0000 \t alpha: 0.1724\n",
      "[22/131][250/381]\tLoss_D: -112488608.0000\tLoss_G: 99662872.0000\tD(x): 13146500.0000\tD(G(z)): -99342104.0000 / -99662872.0000 \t alpha: 0.1734\n",
      "[22/131][300/381]\tLoss_D: -112443600.0000\tLoss_G: -13091016.0000\tD(x): 125568104.0000\tD(G(z)): 13124505.0000 / 13091016.0000 \t alpha: 0.1744\n",
      "[22/131][350/381]\tLoss_D: -224864304.0000\tLoss_G: 211851488.0000\tD(x): 12838384.0000\tD(G(z)): -212025920.0000 / -211851488.0000 \t alpha: 0.1754\n",
      "[23/131][0/381]\tLoss_D: -73788.0000\tLoss_G: -50656188.0000\tD(x): 50454712.0000\tD(G(z)): 50380924.0000 / 50656188.0000 \t alpha: 0.1761\n",
      "[23/131][50/381]\tLoss_D: 75014720.0000\tLoss_G: -125135304.0000\tD(x): 50745700.0000\tD(G(z)): 125760424.0000 / 125135304.0000 \t alpha: 0.1771\n",
      "[23/131][100/381]\tLoss_D: -112783200.0000\tLoss_G: 24607660.0000\tD(x): 88307808.0000\tD(G(z)): -24475392.0000 / -24607660.0000 \t alpha: 0.1781\n",
      "[23/131][150/381]\tLoss_D: -18780.0000\tLoss_G: 24436014.0000\tD(x): -24356404.0000\tD(G(z)): -24375184.0000 / -24436014.0000 \t alpha: 0.1791\n",
      "[23/131][200/381]\tLoss_D: -112871824.0000\tLoss_G: 62200156.0000\tD(x): 50928024.0000\tD(G(z)): -61943796.0000 / -62200156.0000 \t alpha: 0.1801\n",
      "[23/131][250/381]\tLoss_D: -75252872.0000\tLoss_G: 62010304.0000\tD(x): 13259595.0000\tD(G(z)): -61993280.0000 / -62010304.0000 \t alpha: 0.1811\n",
      "[23/131][300/381]\tLoss_D: 37545944.0000\tLoss_G: -88382752.0000\tD(x): 50905816.0000\tD(G(z)): 88451760.0000 / 88382752.0000 \t alpha: 0.1821\n",
      "[23/131][350/381]\tLoss_D: -37693152.0000\tLoss_G: -50573276.0000\tD(x): 88592336.0000\tD(G(z)): 50899184.0000 / 50573276.0000 \t alpha: 0.1831\n",
      "[24/131][0/381]\tLoss_D: -112902360.0000\tLoss_G: 24946620.0000\tD(x): 87601792.0000\tD(G(z)): -25300568.0000 / -24946620.0000 \t alpha: 0.1837\n",
      "[24/131][50/381]\tLoss_D: -75429680.0000\tLoss_G: 138272480.0000\tD(x): -62438700.0000\tD(G(z)): -137868384.0000 / -138272480.0000 \t alpha: 0.1847\n",
      "[24/131][100/381]\tLoss_D: -171018.0000\tLoss_G: 25249520.0000\tD(x): -24380290.0000\tD(G(z)): -24551308.0000 / -25249520.0000 \t alpha: 0.1857\n",
      "[24/131][150/381]\tLoss_D: -75530416.0000\tLoss_G: 100208880.0000\tD(x): -24397250.0000\tD(G(z)): -99927664.0000 / -100208880.0000 \t alpha: 0.1867\n",
      "[24/131][200/381]\tLoss_D: -37987232.0000\tLoss_G: -88604048.0000\tD(x): 126442616.0000\tD(G(z)): 88455384.0000 / 88604048.0000 \t alpha: 0.1877\n",
      "[24/131][250/381]\tLoss_D: -75513896.0000\tLoss_G: 62489424.0000\tD(x): 13060876.0000\tD(G(z)): -62453020.0000 / -62489424.0000 \t alpha: 0.1887\n",
      "[24/131][300/381]\tLoss_D: 375986624.0000\tLoss_G: -106084008.0000\tD(x): -213955136.0000\tD(G(z)): 162031472.0000 / 106084008.0000 \t alpha: 0.1897\n",
      "[24/131][350/381]\tLoss_D: -113448848.0000\tLoss_G: 63167448.0000\tD(x): 50930880.0000\tD(G(z)): -62517972.0000 / -63167448.0000 \t alpha: 0.1907\n",
      "[25/131][0/381]\tLoss_D: -150512416.0000\tLoss_G: 62611936.0000\tD(x): 86712240.0000\tD(G(z)): -63800168.0000 / -62611936.0000 \t alpha: 0.1914\n",
      "[25/131][50/381]\tLoss_D: -75771168.0000\tLoss_G: 177125600.0000\tD(x): -100152992.0000\tD(G(z)): -175924160.0000 / -177125600.0000 \t alpha: 0.1924\n",
      "[25/131][100/381]\tLoss_D: -210338.0000\tLoss_G: -11923080.0000\tD(x): 12955970.0000\tD(G(z)): 12745632.0000 / 11923080.0000 \t alpha: 0.1934\n",
      "[25/131][150/381]\tLoss_D: -38101320.0000\tLoss_G: 63022780.0000\tD(x): -24500286.0000\tD(G(z)): -62601608.0000 / -63022780.0000 \t alpha: 0.1944\n",
      "[25/131][200/381]\tLoss_D: -76012888.0000\tLoss_G: -13202992.0000\tD(x): 89191760.0000\tD(G(z)): 13178872.0000 / 13202992.0000 \t alpha: 0.1954\n",
      "[25/131][250/381]\tLoss_D: -114005504.0000\tLoss_G: 62821280.0000\tD(x): 51258248.0000\tD(G(z)): -62747252.0000 / -62821280.0000 \t alpha: 0.1964\n",
      "[25/131][300/381]\tLoss_D: -151949568.0000\tLoss_G: 62752508.0000\tD(x): 89312920.0000\tD(G(z)): -62636640.0000 / -62752508.0000 \t alpha: 0.1974\n",
      "[25/131][350/381]\tLoss_D: -297887.0000\tLoss_G: -11218599.0000\tD(x): 13210817.0000\tD(G(z)): 12912930.0000 / 11218599.0000 \t alpha: 0.1984\n",
      "[26/131][0/381]\tLoss_D: 76010960.0000\tLoss_G: -12166563.0000\tD(x): -62958620.0000\tD(G(z)): 13052336.0000 / 12166563.0000 \t alpha: 0.1990\n",
      "[26/131][50/381]\tLoss_D: 113375448.0000\tLoss_G: -85494024.0000\tD(x): -25670946.0000\tD(G(z)): 87704504.0000 / 85494024.0000 \t alpha: 0.2000\n",
      "[26/131][100/381]\tLoss_D: -38392392.0000\tLoss_G: -10440236.0000\tD(x): 51406304.0000\tD(G(z)): 13013912.0000 / 10440236.0000 \t alpha: 0.2010\n",
      "[26/131][150/381]\tLoss_D: -98076864.0000\tLoss_G: 183044848.0000\tD(x): -175202368.0000\tD(G(z)): -273279232.0000 / -183044848.0000 \t alpha: 0.2020\n",
      "[26/131][200/381]\tLoss_D: -46107912.0000\tLoss_G: 27284102.0000\tD(x): 14902848.0000\tD(G(z)): -31205064.0000 / -27284102.0000 \t alpha: 0.2030\n",
      "[26/131][250/381]\tLoss_D: 31426248.0000\tLoss_G: -36650416.0000\tD(x): 9237640.0000\tD(G(z)): 40663888.0000 / 36650416.0000 \t alpha: 0.2040\n",
      "[26/131][300/381]\tLoss_D: 1002242.0000\tLoss_G: -11309705.0000\tD(x): 10190794.0000\tD(G(z)): 11193036.0000 / 11309705.0000 \t alpha: 0.2050\n",
      "[26/131][350/381]\tLoss_D: -152012480.0000\tLoss_G: 25312832.0000\tD(x): 126701024.0000\tD(G(z)): -25311456.0000 / -25312832.0000 \t alpha: 0.2060\n",
      "[27/131][0/381]\tLoss_D: -126266928.0000\tLoss_G: 320931328.0000\tD(x): 12401268.0000\tD(G(z)): -113865656.0000 / -320931328.0000 \t alpha: 0.2067\n",
      "[27/131][50/381]\tLoss_D: -77849352.0000\tLoss_G: 99427288.0000\tD(x): -22329582.0000\tD(G(z)): -100178936.0000 / -99427288.0000 \t alpha: 0.2077\n",
      "[27/131][100/381]\tLoss_D: 960448.0000\tLoss_G: -89376640.0000\tD(x): 88698816.0000\tD(G(z)): 89659264.0000 / 89376640.0000 \t alpha: 0.2087\n",
      "[27/131][150/381]\tLoss_D: -458166528.0000\tLoss_G: 254823232.0000\tD(x): 203440272.0000\tD(G(z)): -254726272.0000 / -254823232.0000 \t alpha: 0.2097\n",
      "[27/131][200/381]\tLoss_D: -153296672.0000\tLoss_G: 103161536.0000\tD(x): 50893272.0000\tD(G(z)): -102403408.0000 / -103161536.0000 \t alpha: 0.2107\n",
      "[27/131][250/381]\tLoss_D: -114508336.0000\tLoss_G: 25292504.0000\tD(x): 89048712.0000\tD(G(z)): -25459624.0000 / -25292504.0000 \t alpha: 0.2117\n",
      "[27/131][300/381]\tLoss_D: 114274128.0000\tLoss_G: -89016752.0000\tD(x): -25061164.0000\tD(G(z)): 89212960.0000 / 89016752.0000 \t alpha: 0.2127\n",
      "[27/131][350/381]\tLoss_D: -152884672.0000\tLoss_G: 101803728.0000\tD(x): 51093184.0000\tD(G(z)): -101791480.0000 / -101803728.0000 \t alpha: 0.2137\n",
      "[28/131][0/381]\tLoss_D: -287880.0000\tLoss_G: -50608844.0000\tD(x): 51218032.0000\tD(G(z)): 50930152.0000 / 50608844.0000 \t alpha: 0.2143\n",
      "[28/131][50/381]\tLoss_D: -152749088.0000\tLoss_G: 140048512.0000\tD(x): 12569595.0000\tD(G(z)): -140179488.0000 / -140048512.0000 \t alpha: 0.2153\n",
      "[28/131][100/381]\tLoss_D: -38284180.0000\tLoss_G: 102277672.0000\tD(x): -63841332.0000\tD(G(z)): -102125512.0000 / -102277672.0000 \t alpha: 0.2163\n",
      "[28/131][150/381]\tLoss_D: -115385264.0000\tLoss_G: 64650920.0000\tD(x): 51282568.0000\tD(G(z)): -64102692.0000 / -64650920.0000 \t alpha: 0.2173\n",
      "[28/131][200/381]\tLoss_D: -345344.0000\tLoss_G: -12455828.0000\tD(x): 13126268.0000\tD(G(z)): 12780924.0000 / 12455828.0000 \t alpha: 0.2183\n",
      "[28/131][250/381]\tLoss_D: -229978720.0000\tLoss_G: 178744096.0000\tD(x): 51106744.0000\tD(G(z)): -178871968.0000 / -178744096.0000 \t alpha: 0.2193\n",
      "[28/131][300/381]\tLoss_D: 768316.0000\tLoss_G: 25663348.0000\tD(x): -25607236.0000\tD(G(z)): -24838920.0000 / -25663348.0000 \t alpha: 0.2203\n",
      "[28/131][350/381]\tLoss_D: -38487152.0000\tLoss_G: 63644936.0000\tD(x): -25153698.0000\tD(G(z)): -63640852.0000 / -63644936.0000 \t alpha: 0.2214\n",
      "[29/131][0/381]\tLoss_D: -76876944.0000\tLoss_G: 25647740.0000\tD(x): 51414416.0000\tD(G(z)): -25462528.0000 / -25647740.0000 \t alpha: 0.2220\n",
      "[29/131][50/381]\tLoss_D: -192512288.0000\tLoss_G: 140710720.0000\tD(x): 51869924.0000\tD(G(z)): -140642368.0000 / -140710720.0000 \t alpha: 0.2230\n",
      "[29/131][100/381]\tLoss_D: -115465680.0000\tLoss_G: 25049616.0000\tD(x): 90442256.0000\tD(G(z)): -25023424.0000 / -25049616.0000 \t alpha: 0.2240\n",
      "[29/131][150/381]\tLoss_D: -115610416.0000\tLoss_G: 63791128.0000\tD(x): 51894932.0000\tD(G(z)): -63715480.0000 / -63791128.0000 \t alpha: 0.2250\n",
      "[29/131][200/381]\tLoss_D: -154122048.0000\tLoss_G: 24944510.0000\tD(x): 129080264.0000\tD(G(z)): -25041780.0000 / -24944510.0000 \t alpha: 0.2260\n",
      "[29/131][250/381]\tLoss_D: -38586840.0000\tLoss_G: -13485826.0000\tD(x): 52075136.0000\tD(G(z)): 13488298.0000 / 13485826.0000 \t alpha: 0.2270\n",
      "[29/131][300/381]\tLoss_D: -154480336.0000\tLoss_G: -13492496.0000\tD(x): 167899872.0000\tD(G(z)): 13419536.0000 / 13492496.0000 \t alpha: 0.2280\n",
      "[29/131][350/381]\tLoss_D: -93762.0000\tLoss_G: -13032733.0000\tD(x): 13371576.0000\tD(G(z)): 13277814.0000 / 13032733.0000 \t alpha: 0.2290\n",
      "[30/131][0/381]\tLoss_D: -116233808.0000\tLoss_G: 64565464.0000\tD(x): 51933548.0000\tD(G(z)): -64300264.0000 / -64565464.0000 \t alpha: 0.2296\n",
      "[30/131][50/381]\tLoss_D: -155365536.0000\tLoss_G: 66280552.0000\tD(x): 89953968.0000\tD(G(z)): -65411576.0000 / -66280552.0000 \t alpha: 0.2306\n",
      "[30/131][100/381]\tLoss_D: -76994896.0000\tLoss_G: 25964184.0000\tD(x): 51219552.0000\tD(G(z)): -25775340.0000 / -25964184.0000 \t alpha: 0.2316\n",
      "[30/131][150/381]\tLoss_D: 77436720.0000\tLoss_G: 64609328.0000\tD(x): -141775392.0000\tD(G(z)): -64338676.0000 / -64609328.0000 \t alpha: 0.2326\n",
      "[30/131][200/381]\tLoss_D: -92784.0000\tLoss_G: 25444128.0000\tD(x): -24929808.0000\tD(G(z)): -25022592.0000 / -25444128.0000 \t alpha: 0.2336\n",
      "[30/131][250/381]\tLoss_D: -38463488.0000\tLoss_G: 25135176.0000\tD(x): 13660658.0000\tD(G(z)): -24802828.0000 / -25135176.0000 \t alpha: 0.2347\n",
      "[30/131][300/381]\tLoss_D: -116469888.0000\tLoss_G: 63890296.0000\tD(x): 52574840.0000\tD(G(z)): -63895048.0000 / -63890296.0000 \t alpha: 0.2357\n",
      "[30/131][350/381]\tLoss_D: -271316160.0000\tLoss_G: 102706192.0000\tD(x): 168547712.0000\tD(G(z)): -102768464.0000 / -102706192.0000 \t alpha: 0.2367\n",
      "[31/131][0/381]\tLoss_D: 116470248.0000\tLoss_G: -130057168.0000\tD(x): 13528030.0000\tD(G(z)): 129998280.0000 / 130057168.0000 \t alpha: 0.2373\n",
      "[31/131][50/381]\tLoss_D: -155205856.0000\tLoss_G: 141597264.0000\tD(x): 13580468.0000\tD(G(z)): -141625392.0000 / -141597264.0000 \t alpha: 0.2383\n",
      "[31/131][100/381]\tLoss_D: -38822932.0000\tLoss_G: 25162376.0000\tD(x): 13547928.0000\tD(G(z)): -25275004.0000 / -25162376.0000 \t alpha: 0.2393\n",
      "[31/131][150/381]\tLoss_D: 38918936.0000\tLoss_G: 25243454.0000\tD(x): -63987368.0000\tD(G(z)): -25068434.0000 / -25243454.0000 \t alpha: 0.2403\n",
      "[31/131][200/381]\tLoss_D: -85720.0000\tLoss_G: -52713272.0000\tD(x): 52818876.0000\tD(G(z)): 52733156.0000 / 52713272.0000 \t alpha: 0.2413\n",
      "[31/131][250/381]\tLoss_D: -39017968.0000\tLoss_G: -13837504.0000\tD(x): 52832024.0000\tD(G(z)): 13814056.0000 / 13837504.0000 \t alpha: 0.2423\n",
      "[31/131][300/381]\tLoss_D: 155858144.0000\tLoss_G: -130797376.0000\tD(x): -25057888.0000\tD(G(z)): 130800256.0000 / 130797376.0000 \t alpha: 0.2433\n",
      "[31/131][350/381]\tLoss_D: -155886848.0000\tLoss_G: 64229712.0000\tD(x): 91549312.0000\tD(G(z)): -64337544.0000 / -64229712.0000 \t alpha: 0.2443\n",
      "[32/131][0/381]\tLoss_D: -156098992.0000\tLoss_G: 25054232.0000\tD(x): 131071280.0000\tD(G(z)): -25027712.0000 / -25054232.0000 \t alpha: 0.2449\n",
      "[32/131][50/381]\tLoss_D: -78113920.0000\tLoss_G: 25143368.0000\tD(x): 52983956.0000\tD(G(z)): -25129964.0000 / -25143368.0000 \t alpha: 0.2459\n",
      "[32/131][100/381]\tLoss_D: -117234024.0000\tLoss_G: 25189588.0000\tD(x): 92057928.0000\tD(G(z)): -25176096.0000 / -25189588.0000 \t alpha: 0.2469\n",
      "[32/131][150/381]\tLoss_D: -195388784.0000\tLoss_G: 220583776.0000\tD(x): -25061100.0000\tD(G(z)): -220449888.0000 / -220583776.0000 \t alpha: 0.2480\n",
      "[32/131][200/381]\tLoss_D: -234461344.0000\tLoss_G: 103337792.0000\tD(x): 131119320.0000\tD(G(z)): -103342032.0000 / -103337792.0000 \t alpha: 0.2490\n"
     ]
    }
   ],
   "source": [
    "# Training Loop\n",
    "\n",
    "# Lists to keep track of progress\n",
    "img_list = []\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "iters = 0\n",
    "\n",
    "flag = [64, \"transition\"]\n",
    "# alpha = (epoch_s+1)/num_epochs\n",
    "alpha = 0\n",
    "\n",
    "print(\"Starting Training Loop...\")\n",
    "# For each epoch\n",
    "# for epoch in range(epoch_s + 1, num_epochs):\n",
    "for epoch in range(num_epochs):\n",
    "    # For each batch in the dataloader\n",
    "    for i, data in enumerate(dataloader, 0):\n",
    "        if flag[1] == \"stable\":\n",
    "            pass\n",
    "        else:\n",
    "            alpha = min(1, alpha + 1 / (a_step * num_epochs) )\n",
    "        \n",
    "        ############################\n",
    "        # (1) Update D network: maximize log(D(x)) + log(1 - D(G(z)))\n",
    "        ####copyright####################\n",
    "        ## Train with all-real batch\n",
    "        netD.zero_grad()\n",
    "        # Format batch\n",
    "        real_cpu = data[0].to(device)\n",
    "        y_onehot = turn_label_to_one_hot(data[1].to(device))\n",
    "        b_size = real_cpu.size(0)\n",
    "        label = torch.full((b_size,), real_label, device=device)\n",
    "        # Forward pass real batch through D\n",
    "        output = netD(real_cpu, y_onehot, flag = flag, alpha=alpha).view(-1)\n",
    "        # Calculate loss on all-real batch\n",
    "#         errD_real = criterion(output, label)\n",
    "        errD_real = - output.mean()\n",
    "        # Calculate gradients for D in backward pass\n",
    "        errD_real.backward()\n",
    "        D_x = output.mean().item()\n",
    "        \n",
    "\n",
    "        ## Train with all-fake batch\n",
    "        # Generate batch of latent vectors\n",
    "#         noise = torch.randn(b_size, nz, 1, 1, device=device)\n",
    "        z_fake, y_onehot_fake = gen_fake(b_size)\n",
    "        # Generate fake image batch with G\n",
    "        fake = netG(z_fake, flag = flag, alpha=alpha)\n",
    "        label.fill_(fake_label)\n",
    "        # Classify all fake batch with D\n",
    "        output = netD(fake.detach(), y_onehot_fake, flag = flag, alpha=alpha).view(-1)\n",
    "        # Calculate D's loss on the all-fake batch\n",
    "#         errD_fake = - criterion(output, label)\n",
    "        errD_fake = output.mean() #+ 10.0 * get_gp(real_cpu, fake, netD, y_onehot_fake, flag = flag, alpha=alpha )\n",
    "        # Calculate the gradients for this batch\n",
    "        errD_fake.backward()\n",
    "        D_G_z1 = output.mean().item()\n",
    "        # Add the gradients from the all-real and all-fake batches\n",
    "        errD = errD_real + errD_fake\n",
    "        # Update D\n",
    "        optimizerD.step()\n",
    "        \n",
    "#         if errD_fake.item() + errD_real.item() > 5:\n",
    "#             print(\"D fake error is \", errD_fake.item(), \"and  D real error is \", errD_real.item(), \" in i = \", i, \", epoch = \", epoch )\n",
    "            \n",
    "        copy_unroll(data, K=5)\n",
    "\n",
    "        ############################\n",
    "        # (2) Update G network: maximize log(D(G(z)))\n",
    "        ###########################\n",
    "        netG.zero_grad()\n",
    "        label.fill_(real_label)  # fake labels are real for generator cost\n",
    "        # Since we just updated D, perform another forward pass of all-fake batch through D\n",
    "#         output = netD(fake, flag = flag, alpha=alpha).view(-1)\n",
    "        output = netDK(fake, y_onehot_fake, flag = flag, alpha=alpha).view(-1)\n",
    "        # Calculate G's loss based on this output\n",
    "#         errG = criterion(output, label)\n",
    "        errG = - output.mean()\n",
    "        # Calculate gradients for G\n",
    "        errG.backward()\n",
    "        D_G_z2 = output.mean().item()\n",
    "        # Update G\n",
    "        optimizerG.step()\n",
    "        \n",
    "#         if errG.item() > 5:\n",
    "#             print(\"G error is \", errG.item(), \" in i = \", i, \", epoch = \", epoch )\n",
    "        \n",
    "        # Output training stats\n",
    "        if i % 50 == 0:\n",
    "            print('[%d/%d][%d/%d]\\tLoss_D: %.4f\\tLoss_G: %.4f\\tD(x): %.4f\\tD(G(z)): %.4f / %.4f \\t alpha: %.4f'\n",
    "                  % (epoch, num_epochs, i, len(dataloader),\n",
    "                     errD.item(), errG.item(), D_x, D_G_z1, D_G_z2, alpha))\n",
    "            writer.add_scalar('cPGGANS Discriminator loss {}-{}'.format(flag[0], flag[1]),\n",
    "                            errD.item(),\n",
    "                            epoch * len(dataloader) + i)\n",
    "            writer.add_scalar('cPGGANS Generator loss {}-{}'.format(flag[0], flag[1]),\n",
    "                            errG.item(),\n",
    "                            epoch * len(dataloader) + i)\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, flag = flag, alpha=alpha).detach().cpu()\n",
    "                fake_2 = (fake - fake.min())/(fake.max() - fake.min())\n",
    "                real_cpu_2 = (real_cpu - real_cpu.min())/(real_cpu.max() - real_cpu.min())\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "            writer.add_image('cPGGANS real{}-{}'.format(flag[0], flag[1]),\n",
    "                torchvision.utils.make_grid(real_cpu_2),\n",
    "                global_step=epoch * len(dataloader) + i)\n",
    "            writer.add_image('cPGGANS fake{}-{}'.format(flag[0], flag[1]),\n",
    "                torchvision.utils.make_grid(fake_2),\n",
    "                global_step=epoch * len(dataloader) + i)\n",
    "           \n",
    "        \n",
    "        # Save Losses for plotting later\n",
    "        G_losses.append(errG.item())\n",
    "        D_losses.append(errD.item())\n",
    "        \n",
    "        # Check how the generator is doing by saving G's output on fixed_noise\n",
    "        if (iters % 500 == 0) or ((epoch == num_epochs-1) and (i == len(dataloader)-1)):\n",
    "            with torch.no_grad():\n",
    "                fake = netG(fixed_noise, flag = flag, alpha=alpha).detach().cpu()\n",
    "            img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n",
    "\n",
    "            \n",
    "        iters += 1\n",
    "    if epoch % 25 == 24:\n",
    "        if flag[1] == \"stable\":\n",
    "#             save_model(netG, 'Gen', flag[0], alpha, epoch, 0)\n",
    "#             save_model(netD, 'Dis', flag[0], alpha, epoch, 0)\n",
    "\n",
    "            save_model2(netG,'Gen', flag[0], optimizerG, errG, epoch, 0, SAVE_DIR = \"/home/jovyan/GANs/trained_models_r/\")\n",
    "            save_model2(netD,'Dis', flag[0], optimizerD, errD, epoch, 0, SAVE_DIR = \"/home/jovyan/GANs/trained_models_r/\")\n",
    "        else:\n",
    "#             save_model(netG, 'Gen', flag[0], alpha, epoch, 1)\n",
    "#             save_model(netD, 'Dis', flag[0], alpha, epoch, 1)\n",
    "\n",
    "            save_model2(netG,'Gen', flag[0], optimizerG, errG, epoch, 1, SAVE_DIR = \"/home/jovyan/GANs/trained_models_r/\")\n",
    "            save_model2(netD,'Dis', flag[0], optimizerD, errD, epoch, 1, SAVE_DIR = \"/home/jovyan/GANs/trained_models_r/\")\n",
    "    \n",
    "        \n",
    "#         if i > nsamples:\n",
    "#             break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "plt.title(\"Generator and Discriminator Loss During Training\")\n",
    "plt.plot(G_losses,label=\"G\")\n",
    "plt.plot(D_losses,label=\"D\")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab a batch of real images from the dataloader\n",
    "real_batch = next(iter(dataloader))\n",
    "\n",
    "# Plot the real images\n",
    "plt.figure(figsize=(15,15))\n",
    "plt.subplot(1,2,1)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Real Images\")\n",
    "plt.imshow(np.transpose(vutils.make_grid(real_batch[0].to(device)[:64], padding=5, normalize=True).cpu(),(1,2,0)))\n",
    "\n",
    "# Plot the fake images from the last epoch\n",
    "plt.subplot(1,2,2)\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Fake Images\")\n",
    "plt.imshow(np.transpose(img_list[-1],(1,2,0)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SAVE SNAPSHOTS\n",
    "\n",
    "flag = [32, \"transition\"]\n",
    "alpha = 1\n",
    "idx = 4\n",
    "s=64\n",
    "y_onehot = turn_label_to_one_hot(torch.LongTensor([idx]).expand(s).to(device))\n",
    "y_onehot.shape\n",
    "a_im = netG(torch.cat((torch.randn(s, nz, 1, 1, device=device),y_onehot.view(-1,5,1,1)),dim=1), flag = flag, alpha=alpha)\n",
    "print(a_im.shape)\n",
    "a_grid = vutils.make_grid(a_im, normalize=True)#[0,:,:]\n",
    "a_grid.shape\n",
    "plt.imshow(a_grid.detach().cpu().numpy().transpose(1,2,0))\n",
    "# plt.show()\n",
    "plt.savefig(\"/PATH/TO/Figs/cPGGANs-32-transition-49-RP=4.jpg\",bbox_inches='tight', pad_inches=0,dpi=32*8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
